{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d6b5c0a37147f0991d10aecde212bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "frames=md.load_xtc(\"./singlesim/it50k/eql2.xtc\",top=\"./singlesim/it50k/conf.gro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3576f08c9fb246a8bbc78afe6df12cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# showing the molecule\n",
    "molecule = frames[0]\n",
    "atomic_numbers = [atom.element.atomic_number for atom in molecule.top.atoms]\n",
    "water = Atoms(positions=molecule.xyz[0], numbers=atomic_numbers)\n",
    "show_ase(water)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50001/50001 [00:01<00:00, 36553.62it/s]\n"
     ]
    }
   ],
   "source": [
    "features=[]\n",
    "for molecule in tqdm(frames):\n",
    "    atomic_numbers = np.array([[atom.element.atomic_number for atom in molecule.top.atoms]]).T\n",
    "    vdwr = np.array([[atom.element.radius for atom in molecule.top.atoms]]).T\n",
    "    mass = np.array([[atom.element.mass for atom in molecule.top.atoms]]).T\n",
    "    positions = molecule.xyz[0]*10\n",
    "    \n",
    "    positions = positions - positions[0]\n",
    "    \n",
    "    node_features = np.concatenate((positions,vdwr,atomic_numbers),axis=1)\n",
    "    features.append(node_features)\n",
    "    \n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Features:\n",
      " [[ 0.          0.          0.          0.152       8.        ]\n",
      " [ 0.928936   -0.22790241  0.03701782  0.12        1.        ]\n",
      " [-0.43196297 -0.800951   -0.29684734  0.12        1.        ]\n",
      " [ 0.05301476 -0.10975456 -0.02771759  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Node Features:\\n\",features[50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edge list\n",
    "from_list = []\n",
    "\n",
    "to_list = []\n",
    "for edge in frames.topology.bonds:\n",
    "    from_list.append(edge.atom1.index)\n",
    "    to_list.append(edge.atom2.index)\n",
    "    from_list.append(edge.atom2.index)\n",
    "    to_list.append(edge.atom1.index)\n",
    "\n",
    "edge_list = np.array([from_list,to_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_adj(edge_index, num_nodes=None):\n",
    "    if num_nodes is None:\n",
    "        num_nodes = edge_index.max() + 1\n",
    "    adj = torch.zeros(num_nodes, num_nodes)\n",
    "    adj[edge_index[0], edge_index[1]] = 1\n",
    "    return adj\n",
    "\n",
    "def convert_to_edge_index(adj):\n",
    "    edge_index = adj.nonzero().t()\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50001/50001 [00:00<00:00, 55735.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for node_feature in tqdm(features):\n",
    "    graph = data.Data(x=torch.from_numpy(node_feature),edge_index=torch.from_numpy(edge_list))\n",
    "    graphs.append(graph)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4, 5], edge_index=[2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis = to_networkx(graphs[0])\n",
    "nx.draw_networkx(vis, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50001/50001 [00:00<00:00, 190476.51it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "graphs_device = []\n",
    "for graph in tqdm(graphs):\n",
    "    graphs_device.append(graph.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "in_channels = graph.num_features\n",
    "out_channels = 3\n",
    "n_atoms = 20\n",
    "lr = 1e-3\n",
    "n_epochs = 500\n",
    "batch_size=256\n",
    "test_train_split = 0.8\n",
    "model_name = \"IntraGVAE_l3_final.pt\"\n",
    "model_loaded = False\n",
    "force_train = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if os.path.exists(\"./models/\"+model_name) and not force_train:\n",
    "    model = torch.load(\"./models/\"+model_name)\n",
    "    model_loaded = True\n",
    "else:\n",
    "    model = VGAE(VariationalGCNEncoder(in_channels, out_channels,batch_size,n_atoms),\n",
    "                VariationalGCNDecoder(out_channels, in_channels,batch_size,n_atoms))\n",
    "    \n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(test_train_split * len(graphs_device))\n",
    "train_loader = DataLoader(graphs_device[:split], batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(graphs_device[split:], batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n",
      "total_graphs: 50001\n",
      "Graphs in a batch: 256\n",
      "Train Graphs Batches: 157 (Total graphs: 40192)\n",
      "Test Graphs Batches: 40 (Total graphs: 10240)\n",
      "Model Specifics:\n",
      " VGAE(\n",
      "  (encoder): VariationalGCNEncoder(\n",
      "    (conv1): GCNConv(5, 15)\n",
      "    (head_transform1): Linear(45, 15, bias=True)\n",
      "    (bn1): BatchNorm(15)\n",
      "    (conv2): GCNConv(15, 9)\n",
      "    (bn2): BatchNorm(9)\n",
      "    (linear1): Linear(9, 100, bias=True)\n",
      "    (linear2): Linear(100, 4, bias=True)\n",
      "    (transform): Linear(80, 3, bias=True)\n",
      "    (mu): Linear(3, 3, bias=True)\n",
      "    (logstd): Linear(3, 3, bias=True)\n",
      "  )\n",
      "  (decoder): VariationalGCNDecoder(\n",
      "    (inv_transform): Linear(3, 20, bias=True)\n",
      "    (conv1): GCNConv(1, 9)\n",
      "    (bn1): BatchNorm(9)\n",
      "    (conv2): GCNConv(9, 3)\n",
      "    (bn2): BatchNorm(3)\n",
      "    (conv3): GCNConv(3, 3)\n",
      "    (linear1): Linear(3, 512, bias=True)\n",
      "    (linear2): Linear(512, 128, bias=True)\n",
      "    (linear3): Linear(128, 5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Using:\",device )\n",
    "print(\"total_graphs:\", len(graphs_device))\n",
    "print(\"Graphs in a batch:\", batch_size)\n",
    "print(\"Train Graphs Batches:\",len(train_loader),f\"(Total graphs: {len(train_loader)*batch_size})\")\n",
    "print(\"Test Graphs Batches:\",len(test_loader),f\"(Total graphs: {len(test_loader)*batch_size})\")\n",
    "print(\"Model Specifics:\\n\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFxn = torch.nn.MSELoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    model.double()\n",
    "    loss_all = 0\n",
    "    feature_loss_all = 0\n",
    "    edge_loss_all = 0\n",
    "    position_loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        z,encoded_edge_index = model.encode(data.x, data.edge_index)\n",
    "        decoded_x, decoded_edge_index = model.decode(z, encoded_edge_index)\n",
    "        \n",
    "        heavy_indices = torch.where(data.x[:,4] > torch.tensor([1]).to(device))\n",
    "     \n",
    "        \n",
    "        decoded_pos = decoded_x[:,:3]\n",
    "        decoded_features = decoded_x[:,3:]\n",
    "        orignal_pos = data.x[:,:3]\n",
    "        orignal_features = data.x[:,3:]\n",
    "\n",
    "\n",
    "        positionLoss = lossFxn(decoded_pos[heavy_indices], orignal_pos[heavy_indices])\n",
    "        FeatureLoss = lossFxn(decoded_features[heavy_indices], orignal_features[heavy_indices])\n",
    "        loss = lossFxn(decoded_x[heavy_indices], data.x[heavy_indices])\n",
    "        EdgeLoss = lossFxn(decoded_edge_index.float(), data.edge_index)\n",
    "\n",
    "\n",
    "        loss_all +=  float(loss)\n",
    "        feature_loss_all += float(FeatureLoss)\n",
    "        edge_loss_all += float(EdgeLoss)\n",
    "        position_loss_all += float(positionLoss)\n",
    "\n",
    "#         positionLoss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_all / len(train_loader), feature_loss_all / len(train_loader), edge_loss_all / len(train_loader) ,position_loss_all / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFxn = torch.nn.MSELoss()\n",
    "def test():\n",
    "    model.eval()\n",
    "    model.double()\n",
    "    \n",
    "    loss_all = 0\n",
    "    feature_loss_all = 0\n",
    "    edge_loss_all = 0\n",
    "    position_loss_all = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        optimizer.zero_grad()\n",
    "        z,encoded_edge_index = model.encode(data.x, data.edge_index)\n",
    "        decoded_x, decoded_edge_index = model.decode(z, encoded_edge_index)\n",
    "\n",
    "        decoded_pos = decoded_x[:,:3]\n",
    "        decoded_features = decoded_x[:,3:]\n",
    "        orignal_pos = data.x[:,:3]\n",
    "        orignal_features = data.x[:,3:]\n",
    "        \n",
    "        \n",
    "        heavy_indices = torch.where(data.x[:,4] > torch.tensor([1]).to(device))\n",
    "   \n",
    "        \n",
    "        positionLoss = lossFxn(decoded_pos[heavy_indices], orignal_pos[heavy_indices])\n",
    "        FeatureLoss = lossFxn(decoded_features[heavy_indices], orignal_features[heavy_indices])\n",
    "        loss = lossFxn(decoded_x[heavy_indices], data.x[heavy_indices])\n",
    "\n",
    "        EdgeLoss = lossFxn(decoded_edge_index.float(), data.edge_index)\n",
    "        \n",
    "        \n",
    "        loss_all +=  float(loss)\n",
    "        feature_loss_all += float(FeatureLoss)\n",
    "        edge_loss_all += float(EdgeLoss)\n",
    "        position_loss_all += float(positionLoss)\n",
    "\n",
    "\n",
    "\n",
    "    return loss_all / len(test_loader), feature_loss_all / len(test_loader), edge_loss_all / len(test_loader), position_loss_all / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1024x15 and 45x15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/imami/watermodel/10_7/intra.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         train_total_loss, train_feature_loss, train_edge_loss,train_position_loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         test_total_loss, test_feature_loss, test_edge_loss,test_position_loss \u001b[39m=\u001b[39m test()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/imami/watermodel/10_7/intra.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     z,encoded_edge_index \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     decoded_x, decoded_edge_index \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mdecode(z, encoded_edge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/imami/watermodel/10_7/intra.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     heavy_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(data\u001b[39m.\u001b[39mx[:,\u001b[39m4\u001b[39m] \u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/watermodel/10_7/model.py:187\u001b[0m, in \u001b[0;36mVGAE.encode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    186\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__mu__, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__logstd__, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    188\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    189\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__logstd__ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__logstd__\u001b[39m.\u001b[39mclamp(\u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mMAX_LOGSTD)\n\u001b[1;32m    190\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparametrize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__mu__, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__logstd__)\n",
      "File \u001b[0;32m~/anaconda3/envs/3dpytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/watermodel/10_7/model.py:245\u001b[0m, in \u001b[0;36mVariationalGCNEncoder.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_atoms\n\u001b[1;32m    244\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x, edge_index)\n\u001b[0;32m--> 245\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_transform1(x)\n\u001b[1;32m    246\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    248\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/3dpytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/3dpytorch/lib/python3.11/site-packages/torch_geometric/nn/dense/linear.py:132\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1024x15 and 45x15)"
     ]
    }
   ],
   "source": [
    "train_total_losses = []\n",
    "train_feature_losses = []\n",
    "train_edge_losses = []\n",
    "train_position_losses = []\n",
    "\n",
    "test_total_losses = []\n",
    "test_feature_losses = []\n",
    "test_edge_losses = []\n",
    "test_position_losses = []\n",
    "\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "\n",
    "\n",
    "if model_loaded:        \n",
    "    print(\"Pretrained Model Loaded, no training required\")\n",
    "else:\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_total_loss, train_feature_loss, train_edge_loss,train_position_loss = train()\n",
    "        test_total_loss, test_feature_loss, test_edge_loss,test_position_loss = test()\n",
    "        \n",
    "        print(f\"Epoch: {epoch:03d}\")\n",
    "        print(f'\\tTrain:\\tTotal Loss: {train_total_loss:.4f}, Feature Loss: {train_feature_loss:.4f}, Position Loss: {train_position_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        print(f'\\tTest: \\tTotal Loss: {test_total_loss:.4f}, Feature Loss: {test_feature_loss:.4f}, Position Loss: {test_position_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        if(early_stopper.early_stop(test_total_loss)):\n",
    "            print(\"Early Stopping\")\n",
    "            break\n",
    "\n",
    "        train_total_losses.append(train_total_loss)\n",
    "        train_feature_losses.append(train_feature_loss)\n",
    "        train_edge_losses.append(train_edge_loss)\n",
    "        train_position_losses.append(train_position_loss)\n",
    "\n",
    "        test_total_losses.append(test_total_loss)\n",
    "        test_feature_losses.append(test_feature_loss)\n",
    "        test_edge_losses.append(test_edge_loss)\n",
    "        test_position_losses.append(test_position_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save(model,\"./models/\"+model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
